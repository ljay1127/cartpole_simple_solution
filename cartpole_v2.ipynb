{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=2_000)\n",
    "        \n",
    "        self.random = 1\n",
    "        self.random_decay = 0.999\n",
    "        self.random_min = 0.01\n",
    "        \n",
    "        self.discount = 0.95\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(4,)))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(12, activation='relu'))\n",
    "        model.add(Dense(2, activation='linear'))\n",
    "        model.compile(\n",
    "            optimizer=(Adam(learning_rate=0.001)),\n",
    "            loss='mse',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, states):\n",
    "        if random.random() < self.random:\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(states))\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.memory) < 500:\n",
    "            return\n",
    "        \n",
    "        mini_batch = random.sample(self.memory, 32)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in mini_batch])\n",
    "        next_states = np.array([transition[3] for transition in mini_batch])\n",
    "        current_qs = self.model.predict(current_states)\n",
    "        next_qs = self.model.predict(next_states)\n",
    "            \n",
    "        X = current_states\n",
    "        y = []\n",
    "        \n",
    "        for i, obs in enumerate(mini_batch):\n",
    "            current_state = obs[0]\n",
    "            action = obs[1]\n",
    "            reward = obs[2]\n",
    "            next_state = obs[3]\n",
    "            done = obs[4]\n",
    "            \n",
    "            if done:\n",
    "                new_q = reward\n",
    "            else:\n",
    "                new_q = reward + self.discount * np.max(next_qs[i])\n",
    "            \n",
    "            current_qs[i, action] = new_q\n",
    "            \n",
    "            y.append(current_qs)\n",
    "        \n",
    "        self.model.fit(X, np.array(y), verbose=0, shuffle=False)\n",
    "        self.random = self.random * self.random_decay\n",
    "        if self.random < self.random_min:\n",
    "            self.random = self.random_min\n",
    "        \n",
    "    def remember(self, current_state, action, reward, next_state, done):\n",
    "        self.memory.append([current_state, action, reward, next_state, done])\n",
    "        \n",
    "    def save(self):\n",
    "        self.model.save('cartpole_v2.model')\n",
    "        \n",
    "    def load(self):\n",
    "        self.model = tf.keras.models.load_model('cartpole_v2.model')\n",
    "        self.random = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of games: 1633\n",
      "total reward: 194.0\n",
      "random percentage: 0.19932740843000615\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "for _ in range(2000):\n",
    "    dead = False\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    while not dead:\n",
    "        env.render()\n",
    "        \n",
    "        q = agent.get_action(\n",
    "            np.array(observation).reshape(1,4)\n",
    "        )\n",
    "        observation_, reward, done, info = env.step(q)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            reward = -20\n",
    "        agent.remember(observation, q, reward, observation_, done)\n",
    "        observation = observation_\n",
    "\n",
    "        if done:\n",
    "            agent.train()\n",
    "            clear_output(wait=True)\n",
    "            print(f'no. of games: {_ + 1}')\n",
    "            print(f'total reward: {total_reward}')\n",
    "            print(f'random percentage: {agent.random}')\n",
    "            dead = True\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
